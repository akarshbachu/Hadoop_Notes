{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1) How to say if the problem is BigData problem?__\n",
    "-  We need to consider 2 parameters \n",
    "    -  Volume\n",
    "    -  Velocity\n",
    "-  __Volume:__ We need to understand how big is our data at present and in near future\n",
    "-  __ Velocity:__ How speed our data is growing.\n",
    "\n",
    "-  Based on these 2 parameters we can decide if the problem is BigData problem or not i.e for example if we got data of 1TB in 1month and expecting same growth in near future then for one year we will have 12TB of data, if growth rate is exponential then we will have very huge data, which we cant handle with our HDD or SSD, then it is considered as Big Data Problem.\n",
    "\n",
    "__2) Why Hadoop came into picture?__\n",
    "-  Lets, consider a Scenario where we need to get Max closing price of all Stocks which are Listed in NSE, lets suppose we have __1TB__ of data i.e history data of all stocks, if we are handling with normal Laptop or Desktop and if Data is present on the network, we need to get that to local drive it takes hours of time lets say 3+ hours approx and computation time may be around 1+ hours that totals to 4+ hours, its very huge in Business point of view, if we want to get this done in minutes, we need to Consider Hadoop, because Hadoop converts the data into batches/chunks and distributes this fixed size blocks to different nodes/instances and replicates these blocks in multiple nodes for fail safe and maintains info on which node has which blocks, then it parlallely computes Max Stock price of stocks in the instances/nodes present in the network, which gives the final result in very less time.  \n",
    "\n",
    "__3) Why we need different file system HDFS(Hadoop Distributed File System )?__\n",
    "-  The traditional File Systems like __NTFS__(used in windows, which can handle __16Exa Bytes(1 EB= 1000 Peta Bytes)__ of data), __EXT4__(used in Linux, which can handle __1 Exa Byte__ of data), but in these systems we __doesn't have Image of global distribution of Blocks in the Nodes/instances__ i.e we will not have info on which node has which block info and which block is replicated in which other node, so __this is handled by__ __Hadoop Distributed File System(HDFS)__\n",
    "\n",
    "-  HDFS takes care of distributing Data to multiple nodes in fixed size __Block__ of __128MB__ each\n",
    "-  HDFS takes care of __replicating__ the data in __Multiple Nodes(by default 3 nodes)__ for __Fail Safe__\n",
    "-  HDFS keeps track of info on which node has which block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Benefits of HDFS__\n",
    "-  Supports Distributed Processing\n",
    "-  Handle Failures(by replicating data in multiple nodes)\n",
    "-  Scalability i.e we can add nodes depending on requirements\n",
    "-  Cost Effective i.e we dont need any super special computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is MapReduce?__\n",
    "-  Distributed programming model for processing large data sets\n",
    "-  Created by Google\n",
    "-  Can be implemented in any programming language\n",
    "-  MapReduce is not a programming language\n",
    "-  Hadoop implements MapReduce\n",
    "-  It manage communication, data transfers, parallel execution across distributed servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
